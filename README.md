# Syntactic Generalization Under Constraints: Compact Transformer Models and Limited Linguistic Input

## Abstract

This study examines the syntactic generalization of two compact transformer models, LTG-BERT-Small and LTG-BERT-XS. When trained on limited child-directed datasets from the BabyLM Challenge 2023 and evaluated with BLiMP and its supplement, both models demonstrate the ability to capture some simple syntactic rules but struggle with complex ones. Compared to human and the pre-trained RoBERTa-Base model, the compact models show some gaps, but still have eye-catching performance. Additionally, LTG-BERT-Small outperforming LTG-BERT-XS demonstrates the effect by parameter capacity. By highlighting the strengths of compact models in handling early-acquired linguistic structures and showing their limitations in capturing hierarchical dependencies, this study offers insights into improving syntactic generalization of compact transformer models under resource constraints.
